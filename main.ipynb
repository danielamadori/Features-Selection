{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3a6088",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction to Feature Selection Project\n",
    "\n",
    "This Jupyter notebook is part of a project focused on exploring various feature selection techniques to enhance the performance of machine learning models. The project uses the \"Breast Cancer Wisconsin (Diagnostic)\" dataset, applying methods like Mutual Information Feature Selection (MIFS), Correlation Feature Selection (CFS), and Sequential Forward Selection (SFS) among others, to identify the most significant features for accurate predictions.\n",
    "\n",
    "The goal of this notebook is to provide a comprehensive analysis of these feature selection techniques, compare their effectiveness, and understand their impact on model performance. By the end of this notebook, we should have a clear understanding of which features are most important and why, as well as insights into the strengths and limitations of each feature selection method used.\n",
    "\n",
    "This project is licensed under the MIT License - see the LICENSE file for details."
   ]
  },
  {
   "cell_type": "code",
   "id": "b7aa3da5-85db-469f-8ddf-7a6195e6571d",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from printScore import *\n",
    "import pandas as pd\n",
    "\n",
    "path = 'dataset/'\n",
    "filename = 'breast-cancer-wisconsin.csv'\n",
    "\n",
    "try:\n",
    "\tdf = pd.read_csv(path + filename)\n",
    "except FileNotFoundError:\n",
    "\t# fetch dataset \n",
    "\tbreast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
    "\t\n",
    "\t# data (as pandas dataframes) \n",
    "\tX = breast_cancer_wisconsin_diagnostic.data.features\n",
    "\ty = breast_cancer_wisconsin_diagnostic.data.targets\n",
    "\t\n",
    "\t# Create a Pandas DataFrame with the features and the target\n",
    "\tdf = X.copy()\n",
    "\tdf['target'] = y.copy()\n",
    "\tdf.to_csv(path + filename, index=False)\n",
    "\t\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7cba7bb4b4a1bf0",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "enc.fit(df['target'].values.reshape(-1, 1))\n",
    "df['target'] = enc.transform(df['target'].values.reshape(-1, 1))\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ccd044ef6454a08c",
   "metadata": {},
   "source": [
    "# Discretization with ChiMerge\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Data Preparation**: Begin with continuous numerical data, and sort it in ascending order.\n",
    "\n",
    "2. **Initial Binning**: Create initial bins, typically with a fixed number of data points or fixed width.\n",
    "\n",
    "3. **Calculate Chi-Squared Statistic**: Calculate the chi-squared statistic for each pair of adjacent bins using observed and expected frequencies.\n",
    "\n",
    "4. **Chi-Squared Test**: Apply a chi-squared test to decide whether to merge adjacent bins or keep them separate.\n",
    "\n",
    "5. **Iterate**: Continue merging adjacent bins with chi-squared statistics below the threshold until you reach the desired number of bins or all chi-squared statistics exceed the threshold.\n",
    "\n",
    "6. **Final Binning**: The boundaries of the bins represent the discrete intervals.\n",
    "\n",
    "7. **Discretization**: Replace the continuous data with bin labels to indicate which bin each data point belongs to."
   ]
  },
  {
   "cell_type": "code",
   "id": "9ffb5dfa80886243",
   "metadata": {},
   "source": [
    "from scorecardbundle.feature_discretization import ChiMerge as cm\n",
    "from scorecardbundle.feature_encoding import WOE as woe\n",
    "\n",
    "def chiMerge(df, X_features, y_features, max_intervals=10, min_intervals=2, decimal=None):\n",
    "\ttrans_cm = cm.ChiMerge(max_intervals=max_intervals, min_intervals=min_intervals, decimal=decimal, output_dataframe=True)\n",
    "\n",
    "\tencoder = woe.WOE_Encoder()\n",
    "\tdiscreteDf = pd.DataFrame(\n",
    "\t\tencoder.fit_transform(trans_cm.fit_transform(df[X_features], df[y_features]), df[y_features]),\n",
    "\t\tcolumns=X_features, index=df.index\n",
    "\t)\n",
    "\tdiscreteDf[y_features] = df[y_features]\n",
    "\n",
    "\tuniqueValuesNumber = {feature : len(trans_cm.boundaries_[feature]) for feature in X_features}\n",
    "\t\n",
    "\treturn discreteDf, uniqueValuesNumber, trans_cm.boundaries_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2506a51b3490d55b",
   "metadata": {},
   "source": [
    "# Entropy \n",
    "![H(c) = - Sum(P(c) * log2(P(c)))](images/entropy.PNG)\n",
    "# Joint Entropy\n",
    "![H(C;F) = - Sum(P(c,f) * log2(P(c,f)))](images/jointEntropy.PNG)\n",
    "# Conditional Entropy\n",
    "![H(C|F) = H(C,F) - H(F)](images/conditionalEntropy.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "id": "938b9f4eb2dcd322",
   "metadata": {},
   "source": [
    "from mathFunctions import probability, jointProbabilities\n",
    "\n",
    "# Cache Dictionary\n",
    "entropyCache, jointEntropyCache = {}, {}\n",
    "\n",
    "# H(c) = - Sum(P(c) * log2(P(c)))\n",
    "def entropy(discreteDf, X_feature):\n",
    "\tif X_feature in entropyCache:\n",
    "\t\treturn entropyCache[X_feature]\n",
    "\t\n",
    "\tprob = probability(discreteDf[X_feature].values)\n",
    "\tentropyCache[X_feature] = -np.sum(prob * np.log2(prob))\n",
    "\n",
    "\treturn entropyCache[X_feature]\n",
    "\n",
    "# H(C;F) = - Sum(P(c,f) * log2(P(c,f)))\n",
    "def jointEntropy(discreteDf, X_feature, y_feature):\n",
    "\t# Create a key that doesn't depend on the order of the features\n",
    "\ttmp0, tmp1 = sorted([X_feature, y_feature]) # is a symmetric function\n",
    "\tkey = f\"{tmp0}-{tmp1}\"#\n",
    "\t\n",
    "\tif key in jointEntropyCache:\n",
    "\t\treturn jointEntropyCache[key]\n",
    "\t\n",
    "\tjp= jointProbabilities(discreteDf[X_feature].values, discreteDf[y_feature].values)\n",
    "\tjp = jp[jp > 0]\n",
    "\tjointEntropyCache[key] = - np.sum(jp * np.log2(jp))\n",
    "\t\n",
    "\treturn jointEntropyCache[key]\n",
    "\n",
    "# H(C|F) = H(C,F) - H(F)\n",
    "def conditionalEntropy(discreteDf, X_feature, y_feature):\n",
    "\treturn jointEntropy(discreteDf, X_feature, y_feature) - entropy(discreteDf, y_feature)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d841d1a4eed59a1",
   "metadata": {},
   "source": [
    "# Mutual Information\n",
    "![mutualInformation](images/mutualInformation.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "id": "b510fc340fe67f42",
   "metadata": {},
   "source": [
    "# I_(f;c) = H(f) - H(f|c) Mutual Information function\n",
    "def mutual_information(discreteDf, X_feature, y_feature):\n",
    "\tmi = entropy(discreteDf, X_feature) - conditionalEntropy(discreteDf, X_feature, y_feature)\n",
    "\t\n",
    "\tif mi < 0:\n",
    "\t\tprint(\"Mutual Information error: \")\n",
    "\t\tprint(f\"I({X_feature};{y_feature}) = H({X_feature})-H({X_feature}, {y_feature}) = {entropy(discreteDf, X_feature):.3f} - {conditionalEntropy(discreteDf, X_feature, y_feature):.3f} = {mi:.5f}\")\n",
    "\t\n",
    "\treturn mi\n",
    "\n",
    "#print(\"Finish: \", mutual_information(discreteDf, discreteDf.columns.tolist(), discreteDf.columns[-1]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "24107d551f302ed7",
   "metadata": {},
   "source": [
    "# Evaluation Function: Mutual Information Feature Selection (MIFS)\n",
    "\n",
    "![MIFS_formula](images/MIFS_formula.PNG)\n",
    "\n",
    "Where:\n",
    "- f: feature in remaining features\n",
    "- S: all the selected features\n",
    "- C: the target feature (class)\n",
    "\n",
    "**Reference**: Roberto Battiti, \"Using Mutual Information for Selecting Features in Supervised Neural Net Learning\", IEEE Transactions on Neural Networks, August 1994. DOI: [10.1109/72.298224](https://doi.org/10.1109/72.298224)."
   ]
  },
  {
   "cell_type": "code",
   "id": "827beecb61ef51db",
   "metadata": {},
   "source": [
    "import concurrent.futures\n",
    "#from ipyparallel import Client\n",
    "#c = Client(profile='default')\n",
    "\n",
    "def mifs(discreteDf, selectedFeatures, feature, _class, beta=0.5):\n",
    "\tmiFeatureClass = mutual_information(df, _class, feature) #I_(C;f) = H(C) - H(C|f)\n",
    "\n",
    "\t# I_(f;S) = Sum(I_(f;s)) for each s in S\n",
    "\tif not isinstance(selectedFeatures, list): # Check if y_features is a list\n",
    "\t\tselectedFeatures = [selectedFeatures]\n",
    "\n",
    "\tsum_mi_SelectedFeatures = 0\n",
    "\twith concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor: # Use ThreadPoolExecutor to parallelize the calculations\n",
    "\t\t# Submit tasks for each combination of X_feature and y_feature\n",
    "\t\tfutures = [executor.submit(mutual_information, discreteDf, feature, selectedFeature) for selectedFeature in selectedFeatures]\n",
    "\tfor future in concurrent.futures.as_completed(futures): # Gather the results\n",
    "\t\tsum_mi_SelectedFeatures += future.result()\n",
    "\t#print(f\"\\nI_(f;S) = Sum(I_(f;s)) = {sum}\\n-F = {feature}\\n-S = {selectedFeatures}\\n\")\n",
    "\t\n",
    "\t# I_(C;f) - Beta * (Som(#I_(f;s)) for each s in S)\n",
    "\tresult = miFeatureClass - beta * sum_mi_SelectedFeatures\n",
    "\t#print(f\"I({_class};{feature})-Beta*(Som(I_({feature};s)) for each s in S) ={miFeatureClass:.3} - {beta} * {sum_mi_SelectedFeatures:.3f} = {result:.5f}\")\n",
    "\treturn result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "573cb7069ad60e91",
   "metadata": {},
   "source": [
    "# Evaluation Function: Correlation Feature Selection (CFS)\n",
    "\n",
    "CFS is a filter-based method that evaluates the worth of a subset of features by considering the individual predictive ability of each feature along with the degree of redundancy between them.\n",
    "\n",
    "![CSF_formula](images/CSF_formula.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "id": "88c530fe2b1ec4ab",
   "metadata": {},
   "source": [
    "# Cache Dictionary\n",
    "cfsCache = {}\n",
    "\n",
    "def cfs(df, attributes, _class):\n",
    "    # Create a key that doesn't depend on the order of the features\n",
    "    if not isinstance(attributes, list): # If there is just one string \n",
    "        attributes = [attributes]\n",
    "    else:\n",
    "        attributes = sorted(attributes)\n",
    "\n",
    "    key = f\"{attributes}-{_class}\"\n",
    "\n",
    "    if key in cfsCache:\n",
    "        return cfsCache[key]\n",
    "\n",
    "    k = len(attributes)\n",
    "\n",
    "    avgCorrAttributeClass = 0\n",
    "    avgCorrAttributeAttribute = 0\n",
    "    for attribute in attributes:\n",
    "        # compute the average correlation between the attributes and the class\n",
    "        avgCorrAttributeClass += np.abs(df[attribute].corr(df[_class]))\n",
    "        # compute the average correlation between the attributes\n",
    "        remainingAttributes = attributes.copy()\n",
    "        remainingAttributes.remove(attribute)\n",
    "        for attribute2 in remainingAttributes:\n",
    "            avgCorrAttributeAttribute += np.abs(df[attribute].corr(df[attribute2]))\n",
    "\n",
    "    avgCorrAttributeClass =  avgCorrAttributeClass / k\n",
    "    avgCorrAttributeAttribute = avgCorrAttributeAttribute / (k * k)\n",
    "\n",
    "    # compute the score of the testFeature\n",
    "    cfsCache[key] = (k * avgCorrAttributeClass) / np.sqrt(k + k * (k - 1) * avgCorrAttributeAttribute)\n",
    "    return cfsCache[key]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9f7279d7624d6273",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![variance](images/variance.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c33b18cf1151900d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Cache Dictionary\n",
    "varianceCache = {}\n",
    "\n",
    "def variance(df, feature):\n",
    "    # Use feature as key for the dictionary\n",
    "    if feature in varianceCache:\n",
    "        return varianceCache[feature]\n",
    "\n",
    "    varianceCache[feature] = df[feature].var()\n",
    "    return varianceCache[feature]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "425956359c3d1a48",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![correlation](images/correlation.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "id": "97575c3f80041fd6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Cache Dictionary\n",
    "correlationCache = {}    \n",
    "def correlation(df, feature, target):\n",
    "    # Create a key that doesn't depend on the order of the features\n",
    "    tmp0, tmp1 = sorted([feature, target]) # is a symmetric function\n",
    "    key = f\"{tmp0}-{tmp1}\"\n",
    "\n",
    "    if key in correlationCache:\n",
    "        return correlationCache[key]\n",
    "\n",
    "    correlationCache[key] = np.abs(df[feature].corr(df[target]))\n",
    "    return correlationCache[key]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1e6edab3bc22fe9",
   "metadata": {},
   "source": [
    "## Sequential Forward Selection (SFS)\n",
    "\n",
    "The SFS algorithm is a feature selection method that starts with a single attribute and incrementally adds attributes until the full set of attributes is reached. It is particularly effective when the optimal subset has only a few attributes. The process involves evaluating a large number of states initially, but as it progresses, the region examined by SFS becomes narrower since most of the attributes have already been selected.\n",
    "\n",
    "![SFS](images/SFS.PNG)\n",
    "\n",
    "## Implementation of the SFS Algorithm\n",
    "\n",
    "In the following implementation of the SFS algorithm, we begin with a set of features, which can be either empty or contain initial attributes, and a list of remaining features, which includes all features except those already selected and the target feature. The algorithm operates through a series of iterations:\n",
    "\n",
    "1. Compute the score of each remaining feature considering the selected features and the target feature.\n",
    "2. Select the feature with the best score.\n",
    "3. Add the selected feature to the list of selected features.\n",
    "4. Remove the selected feature from the list of remaining features.\n",
    "\n",
    "This process is repeated until the maximum number of iterations is reached or until there are no more remaining features to select from.\n",
    "\n",
    "### Evaluation Functions for Feature Score\n",
    "\n",
    "The score of each feature is computed using one of the following evaluation functions:\n",
    "\n",
    "- Variance\n",
    "- Correlation\n",
    "- Correlation Feature Selection (CFS)\n",
    "- Mutual Information Feature Selection (MIFS)\n",
    "\n",
    "If multiple evaluation functions are given to the function, the score is based on the metrics order of priority given by the order of the metrics in the metrics list, so if the first score is the same for two features, the second score is used to determine the best feature and so on."
   ]
  },
  {
   "cell_type": "code",
   "id": "7b77cc68459f6575",
   "metadata": {},
   "source": [
    "from metrics import checkMetrics, createFeatureScore, compareFeatureScore\n",
    "from utils import mifs_caching_init, mifs_caching_flush\n",
    "\n",
    "allowedMetrics = ['variance', 'correlation', 'cfs', 'mifs']\n",
    "\n",
    "# MIFS caching\n",
    "discreteDf, entropyCache, jointEntropyCache, discreteDf_filename, entropyFilename, jointEntropyFilename = mifs_caching_init(path, filename, 'discrete.csv', 'entropy.csv', 'joint_entropy.csv')\n",
    "\n",
    "def selection_sfs(df, selectedFeatures, remainingFeatures, target, metrics, maxIteration=np.inf, discreteDf=None, beta=0.5):\n",
    "\tmetrics = checkMetrics(metrics, allowedMetrics)\n",
    "\n",
    "\t# Create the discrete data frame if needed\n",
    "\tmifs_cache = discreteDf is not None\n",
    "\tif allowedMetrics[3] in metrics and discreteDf is None:\n",
    "\t\tprint(\"Warning: mifs cache not enabled\")\n",
    "\t\tdiscreteDf, uniqueValuesNumber, featuresBoundaries = chiMerge(df, df.columns[:-1].tolist(), df.columns[-1], max_intervals=100, min_intervals=2)\n",
    "\t\tdiscreteDf.to_csv(path + discreteDf_filename, index=False)\n",
    "\t\tglobal entropyCache, jointEntropyCache\n",
    "\t\tentropyCache, jointEntropyCache = {}, {} # Cleaning Cache\n",
    "\t\t\n",
    "\t\t#print(\"Features Boundaries:\\n\", featuresBoundaries)\n",
    "\t\t#print(\"Features unique values:\\n\", pd.DataFrame.from_dict(uniqueValuesNumber,orient='index', columns=['Number of unique values']))\n",
    "\t\t#print(\"Discrete data frame:\\n\", discreteDf)\t\t\n",
    "\n",
    "\t# Algorithm\n",
    "\tscore = []\n",
    "\tcounter = 0\n",
    "\twhile counter < maxIteration and len(remainingFeatures) > 0:\n",
    "\t\t# initialize the best score and feature\n",
    "\t\tbestFeature = None\n",
    "\t\tbestScore = createFeatureScore(metrics)\n",
    "\t\t\n",
    "\t\tfor feature in remainingFeatures:\n",
    "\t\t\tfeatureScore = createFeatureScore(metrics)# Initialize the feature score with the right metrics order\n",
    "\t\t\t\n",
    "\t\t\t# compute the variance score\n",
    "\t\t\tif allowedMetrics[0] in metrics: \n",
    "\t\t\t\tfeatureScore[allowedMetrics[0]] = variance(df, feature) \n",
    "\t\t\t# compute the correlation score\n",
    "\t\t\tif allowedMetrics[1] in metrics:\n",
    "\t\t\t\tfeatureScore[allowedMetrics[1]] = correlation(df, feature, target)\n",
    "\t\t\t# compute the cfs score\n",
    "\t\t\tif allowedMetrics[2] in metrics:\n",
    "\t\t\t\tfeatureScore[allowedMetrics[2]] = cfs(df, selectedFeatures + [feature], target)\n",
    "\t\t\t# compute the mifs score\n",
    "\t\t\tif allowedMetrics[3] in metrics:\n",
    "\t\t\t\tfeatureScore[allowedMetrics[3]] = mifs(df, selectedFeatures, feature, target, beta=beta)\n",
    "\t\t\t\tif mifs_cache:\n",
    "\t\t\t\t\tmifs_caching_flush(path, entropyCache, jointEntropyCache, entropyFilename, jointEntropyFilename)\n",
    "\t\t\t\n",
    "\t\t\t# check if the score is better than the best score considering all the metrics with order of priority\n",
    "\t\t\tif compareFeatureScore(featureScore, bestScore, metrics):\n",
    "\t\t\t\tbestScore = featureScore\n",
    "\t\t\t\tbestFeature = feature\n",
    "\t\t\t\n",
    "\t\tselectedFeatures.append(bestFeature)\n",
    "\t\tremainingFeatures.remove(bestFeature)\n",
    "\t\tscore.append(bestScore)\n",
    "\t\tcounter += 1\n",
    "\t\n",
    "\treturn selectedFeatures, score, remainingFeatures"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4a9f461b561d2576",
   "metadata": {},
   "source": [
    "## Sequential Backward Selection (SBS)\n",
    "\n",
    "The SBS algorithm is a feature selection method that begins with the full set of attributes and iteratively removes attributes until only a selected subset of features remains. It is particularly effective when the optimal subset has a large number of attributes. The process starts with the entire feature set and gradually narrows it down by eliminating less important features.\n",
    "\n",
    "![SBS](images/SBS.PNG)\n",
    "\n",
    "## Implementation of the SBS Algorithm\n",
    "\n",
    "In the following implementation of the SBS algorithm, we start with the a set of remaining features and progressively reduce it through a series of iterations:\n",
    "\n",
    "1. Compute for each remaining feature the score of all remaining feature except the evaluated one\n",
    "2. Remove the feature with the lowest score.\n",
    "\n",
    "This process is repeated until the maximum number of iterations is reached or until there is one remaining feature\n",
    "\n",
    "### Evaluation Functions for Feature Score\n",
    "\n",
    "The score of each feature is determined using one of the following evaluation functions:\n",
    "\n",
    "- Variance\n",
    "- Correlation\n",
    "- Correlation Feature Selection (CFS)\n",
    "- Mutual Information Feature Selection (MIFS)\n",
    "\n",
    "If multiple evaluation functions are given to the function, the score is based on the metrics order of priority given by the order of the metrics in the metrics list, so if the first score is the same for two features, the second score is used to determine the best feature and so on."
   ]
  },
  {
   "cell_type": "code",
   "id": "ab200f827a14a207",
   "metadata": {},
   "source": [
    "def selection_sbs(df, remainingFeatures, target, metrics, maxIteration=np.inf, discreteDf=None, beta=0.5):\n",
    "    metrics = checkMetrics(metrics, allowedMetrics)\n",
    "\n",
    "    # Create the discrete data frame if needed\n",
    "    mifs_cache = discreteDf is not None\n",
    "    if allowedMetrics[3] in metrics and discreteDf is None:\n",
    "        print(\"Warning: mifs cache not enabled\")\n",
    "        discreteDf, uniqueValuesNumber, featuresBoundaries = chiMerge(df, df.columns[:-1].tolist(), df.columns[-1], max_intervals=100, min_intervals=2)\n",
    "        discreteDf.to_csv(path + discreteDf_filename, index=False)\n",
    "        global entropyCache, jointEntropyCache\n",
    "        entropyCache, jointEntropyCache = {}, {} # Cleaning Cache\n",
    "\n",
    "    #print(\"Features Boundaries:\\n\", featuresBoundaries)\n",
    "    #print(\"Features unique values:\\n\", pd.DataFrame.from_dict(uniqueValuesNumber,orient='index', columns=['Number of unique values']))\n",
    "    #print(\"Discrete data frame:\\n\", discreteDf)\n",
    "\n",
    "    # Algorithm\n",
    "    score = []\n",
    "    eliminatedFeatures = []\n",
    "    remainingFeatures = remainingFeatures.copy()\n",
    "    while len(eliminatedFeatures) < maxIteration and len(remainingFeatures) > 1:\n",
    "        # initialize the best score and feature\n",
    "        worstFeature = None\n",
    "        worstScore = createFeatureScore(metrics, positiveValue=True)\n",
    "\n",
    "        for feature in remainingFeatures:\n",
    "            featureScore = createFeatureScore(metrics) # Initialize the feature score with the right metrics order\n",
    "\n",
    "            # Select all the features except the current feature\n",
    "            testFeatures = remainingFeatures.copy()\n",
    "            testFeatures.remove(feature)\n",
    "\n",
    "            if allowedMetrics[0] in metrics:\n",
    "                featureScore[allowedMetrics[0]] = variance(df, feature)\n",
    "            if allowedMetrics[1] in metrics:\n",
    "                featureScore[allowedMetrics[1]] = correlation(df, feature, target)\n",
    "            if allowedMetrics[2] in metrics:\n",
    "                featureScore[allowedMetrics[2]] = cfs(df, testFeatures, target)\n",
    "            if allowedMetrics[3] in metrics:\n",
    "                featureScore[allowedMetrics[3]] = mifs(df, testFeatures, feature, target, beta=beta)\n",
    "                if mifs_cache:\n",
    "                    mifs_caching_flush(path, entropyCache, jointEntropyCache, entropyFilename, jointEntropyFilename)\n",
    "\n",
    "            # check if the score is worse than the worst score considering all the metrics with order of priority\n",
    "            if compareFeatureScore(worstScore, featureScore, metrics):\n",
    "                worstScore = featureScore\n",
    "                worstFeature = feature\n",
    "\n",
    "        remainingFeatures.remove(worstFeature)\n",
    "        score.append(worstScore)\n",
    "        eliminatedFeatures.append(worstFeature)\n",
    "\n",
    "    return remainingFeatures, eliminatedFeatures, score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "77c16709d62e5b98",
   "metadata": {},
   "source": [
    "### Applying the SFS and SBS algorithms to the Breast Cancer Wisconsin (Diagnostic) Data Set\n",
    "\n",
    "1. Initialize the selected features list\n",
    "2. Initialize the remaining features list without the target\n",
    "3. Get target name\n",
    "4. Call the SFS function with maxIteration= 2\n",
    "5. Call the SBS function with maxIteration= 3\n",
    "6. Print the results"
   ]
  },
  {
   "cell_type": "code",
   "id": "7cfad075f19f7867",
   "metadata": {},
   "source": [
    "target = df.columns[-1]\n",
    "\n",
    "selectedFeatures, score, remainingFeatures = selection_sfs(df, [], df.columns[:-1].tolist(), target,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   metrics=allowedMetrics[0], maxIteration=2)\n",
    "remainingFeatures, eliminatedFeatures, worstScore = selection_sbs(df, remainingFeatures, target,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  metrics=allowedMetrics[0], maxIteration= 3)\n",
    "\n",
    "#printScoreDetails(selectedFeatures, score, remainingFeatures)\n",
    "printFinalScore(selectedFeatures, score, eliminatedFeatures, worstScore)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "18541a06726f2eb4",
   "metadata": {},
   "source": [
    "## Bidirectional Selection (BDS)\n",
    "\n",
    "The BDS algorithm is a search method that simultaneously explores the search space from both the initial state and the goal state. It is particularly effective in finding the shortest path between two states in a search graph. The process involves two simultaneous searches, one forward (SFS) from the initial state and one backward (SBS) from the goal state, with the goal of meeting in the middle.\n",
    "\n",
    "![BDS](images/BDS.PNG)\n",
    "\n",
    "## Implementation of the BDS Algorithm\n",
    "\n",
    "In the following implementation of the SFS algorithm, we begin with a set of features, which can be either empty or contain initial attributes, and a list of remaining features, which includes all features except those already selected and the target feature. The algorithm operates through a series of iterations:\n",
    "\n",
    "### Sequential Forward Selection (SFS):\n",
    "\n",
    "1. Compute the score of each remaining feature considering the selected features and the target feature.\n",
    "2. Select the feature with the best score.\n",
    "3. Add the selected feature to the list of selected features.\n",
    "4. Remove the selected feature from the list of remaining features.\n",
    "\n",
    "### Sequential Backward Selection (SBS):\n",
    "\n",
    "5. Compute for each remaining feature the score of all remaining features except the evaluated one.\n",
    "6. Remove the feature with the lowest score.\n",
    "\n",
    "This process is repeated until the maximum number of iterations is reached or until there are no more remaining features to select from. The goal of BDS is to minimize the search space by simultaneously exploring from both ends, reducing the time and resources needed to find a solution."
   ]
  },
  {
   "cell_type": "code",
   "id": "13b485cb77318432",
   "metadata": {},
   "source": [
    "def selection_bds(df, selectedFeatures, remainingFeatures, target, metrics, maxIteration=1, discreteDf=None, beta=0.5):\n",
    "\tmetrics = checkMetrics(metrics, allowedMetrics)\n",
    "\t\n",
    "\tselectedFeaturesScore = []\n",
    "\teliminatedFeaturesWorstScore = []\n",
    "\teliminatedFeatures = []\n",
    "\n",
    "\twhile(len(selectedFeatures) < maxIteration and len(remainingFeatures) > 0):\n",
    "\t\tselectedFeatures, selectedFeatureScore, remainingFeatures = selection_sfs(df, selectedFeatures, remainingFeatures, target, metrics, maxIteration= 1, discreteDf=discreteDf, beta=beta)\n",
    "\t\tremainingFeatures, eliminatedFeature, eliminatedFeatureWorstScore = selection_sbs(df, remainingFeatures, target, metrics, maxIteration= 1, discreteDf=discreteDf, beta=beta)\n",
    "\n",
    "\t\tselectedFeaturesScore.extend(selectedFeatureScore)\n",
    "\t\tmaxIterationOverLoad = eliminatedFeature == []#It appends when |remainingFeatures| = 2\n",
    "\t\tif not maxIterationOverLoad:\n",
    "\t\t\teliminatedFeaturesWorstScore.extend(eliminatedFeatureWorstScore)\n",
    "\t\t\teliminatedFeatures.extend(eliminatedFeature)\n",
    "\t\t\n",
    "\treturn selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures, eliminatedFeaturesWorstScore"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8c840a490acd8391",
   "metadata": {},
   "source": [
    "### Bidirectional Selection with Variance as evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "69528fa5a0feb94f",
   "metadata": {},
   "source": [
    "selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures, eliminatedFeaturesWorstScore = selection_bds(df, [], df.columns[:-1].tolist(), target, metrics=allowedMetrics[0], maxIteration= 5)\n",
    "\n",
    "printFinalScore(selectedFeatures, selectedFeaturesScore, eliminatedFeatures, eliminatedFeaturesWorstScore)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c422f130b9521c2",
   "metadata": {},
   "source": [
    "### Bidirectional Selection with Correlation as evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "ffb238e0c7f4c6fa",
   "metadata": {},
   "source": [
    "selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures, eliminatedFeaturesWorstScore = selection_bds(df, [], df.columns[:-1].tolist(), target, metrics=allowedMetrics[1], maxIteration= 5)\n",
    "\n",
    "printFinalScore(selectedFeatures, selectedFeaturesScore, eliminatedFeatures, eliminatedFeaturesWorstScore)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "576dc1b5a94d107e",
   "metadata": {},
   "source": [
    "### Bidirectional Selection with Correlation Feature Selection as evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "5703b7ab02c4272d",
   "metadata": {},
   "source": [
    "selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures, eliminatedFeaturesWorstScore = selection_bds(df, [], df.columns[:-1].tolist(), target, metrics=allowedMetrics[2], maxIteration= 5)\n",
    "\n",
    "printFinalScore(selectedFeatures, selectedFeaturesScore, eliminatedFeatures, eliminatedFeaturesWorstScore)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f37f76d58458460b",
   "metadata": {},
   "source": [
    "### Bidirectional Selection with Mutual Information Feature Selection as evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed51765a25b1d2b7",
   "metadata": {},
   "source": [
    "selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures, eliminatedFeaturesWorstScore = selection_bds(df, [], df.columns[:-1].tolist(), target, metrics=allowedMetrics[3], maxIteration= 5, discreteDf=discreteDf)\n",
    "\n",
    "printFinalScore(selectedFeatures, selectedFeaturesScore, eliminatedFeatures, eliminatedFeaturesWorstScore)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a28241c717261e9b",
   "metadata": {},
   "source": [
    "## Plus-l-Minus-r Selection (LRS)\n",
    "\n",
    "The Plus-l-Minus-r Selection method is like the Bidirectional Selection (BDS) algorithm, but it allows the forward and backward searches to be performed with different step sizes.The l parameter controls the number of features added in each iteration of the forward search (SFS), while the r parameter controls the number of features removed in each iteration of the backward search (SBS). \n",
    "\n",
    "![Plus-l-Minus-r Selection](images/LRS.PNG)\n",
    "\n",
    "### Implementation of the Plus-l-Minus-r Selection Algorithm\n",
    "\n",
    "In the following implementation of the Plus-l-Minus-r Selection algorithm, we begin with a set of features. This set can initially be empty or contain the starting attributes. We also maintain a list of remaining features, which includes all features except those already selected and the target feature. The algorithm operates through a series of iterations:\n",
    "\n",
    "#### Sequential Forward Selection (SFS):\n",
    "\n",
    "1. Calculate the score for each remaining feature, considering the selected features and the target feature.\n",
    "2. Choose the feature with the highest score.\n",
    "3. Add the selected feature to the list of chosen features.\n",
    "4. Remove the selected feature from the list of remaining features.\n",
    "5. Repeat steps 1-4 until l features have been selected.\n",
    "\n",
    "#### Sequential Backward Selection (SBS):\n",
    "\n",
    "6. Calculate the score for each remaining feature, considering all remaining features except the one being evaluated.\n",
    "7. Remove the feature with the lowest score.\n",
    "8. Repeat steps 6-7 until r features have been removed.\n",
    "\n",
    "This process is repeated until the maximum number of iterations is reached or until there are no more remaining features to select from. The goal of Plus-l-Minus-r Selection is to minimize the search space by simultaneously exploring from both ends, reducing the time and resources needed to find a solution."
   ]
  },
  {
   "cell_type": "code",
   "id": "70bf25255ebfbb09",
   "metadata": {},
   "source": [
    "# Implementation of the LRS algorithm using the SFS and SBS function\n",
    "def selection_lrs(df, selectedFeatures, remainingFeatures, target, metrics, l=1, r=1, maxIteration=1, discreteDf=None, beta=0.5):\n",
    "\tmetrics = checkMetrics(metrics, allowedMetrics)\n",
    "\t\n",
    "\tselectedFeaturesScore = []\n",
    "\teliminatedFeaturesWorstScore = []\n",
    "\teliminatedFeatures = []\n",
    "\n",
    "\twhile(len(selectedFeatures) < maxIteration and len(remainingFeatures) > 0):\n",
    "\t\tselectedFeatures, selectedFeatureScore, remainingFeatures = selection_sfs(df, selectedFeatures, remainingFeatures, target, metrics, maxIteration= l, discreteDf=discreteDf, beta=beta)\n",
    "\t\tremainingFeatures, eliminatedFeature, eliminatedFeatureWorstScore = selection_sbs(df, remainingFeatures, target, metrics, maxIteration= r, discreteDf=discreteDf, beta=beta)\n",
    "\n",
    "\t\tselectedFeaturesScore.extend(selectedFeatureScore)\n",
    "\t\tmaxIterationOverLoad = eliminatedFeature == []#It appends when |remainingFeatures| = 2\n",
    "\t\tif not maxIterationOverLoad:\n",
    "\t\t\teliminatedFeaturesWorstScore.extend(eliminatedFeatureWorstScore)\n",
    "\t\t\teliminatedFeatures.extend(eliminatedFeature)\n",
    "\t\n",
    "\treturn selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures, eliminatedFeaturesWorstScore"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d3d34e1118f2904",
   "metadata": {},
   "source": [
    "## Plus-l-Minus-r Selection with Variance as evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "d279c868b5458b6c",
   "metadata": {},
   "source": [
    "selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures, eliminatedFeaturesWorstScore = selection_lrs(df, [], df.columns[:-1].tolist(), df.columns[-1], metrics=allowedMetrics[0], maxIteration= 5, l=2, r=1)\n",
    "\n",
    "printFinalScore(selectedFeatures, selectedFeaturesScore, eliminatedFeatures, eliminatedFeaturesWorstScore)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ac639e6f8a0b613",
   "metadata": {},
   "source": [
    "## Plus-l-Minus-r Selection with Correlation as evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "c256075bc2d9b062",
   "metadata": {},
   "source": [
    "selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures, eliminatedFeaturesWorstScore = selection_lrs(df, [], df.columns[:-1].tolist(), df.columns[-1], metrics=allowedMetrics[1], maxIteration= 5, l=2, r=1)\n",
    "\n",
    "printFinalScore(selectedFeatures, selectedFeaturesScore, eliminatedFeatures, eliminatedFeaturesWorstScore)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "761301d334a243f8",
   "metadata": {},
   "source": [
    "## Plus-l-Minus-r Selection with Correlation Feature Selection as evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "b2a815ef87e33c0b",
   "metadata": {},
   "source": [
    "selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures, eliminatedFeaturesWorstScore = selection_lrs(df, [], df.columns[:-1].tolist(), df.columns[-1], metrics=allowedMetrics[2], maxIteration= 5, l=2, r=1)\n",
    "\n",
    "printFinalScore(selectedFeatures, selectedFeaturesScore, eliminatedFeatures, eliminatedFeaturesWorstScore)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fba579e83f4c8674",
   "metadata": {},
   "source": [
    "## Plus-l-Minus-r Selection with Mutual Information Feature Selection as evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "f014f2c3c09f08ff",
   "metadata": {},
   "source": [
    "selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures, eliminatedFeaturesWorstScore = selection_lrs(df, [], df.columns[:-1].tolist(), df.columns[-1], metrics=allowedMetrics[3], maxIteration= 5, discreteDf=discreteDf, l=2, r=1)\n",
    "\n",
    "printFinalScore(selectedFeatures, selectedFeaturesScore, eliminatedFeatures, eliminatedFeaturesWorstScore)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dbb9b7d879effe0b",
   "metadata": {},
   "source": [
    "## Sequential Forward Floating Selection (SFFS)\n",
    "\n",
    "The Sequential Forward Floating Selection (SFFS) method is a feature selection technique that combines elements of forward selection and backward elimination. SFFS aims to find an optimal subset of features by iteratively adding and removing features based on their performance scores.\n",
    "\n",
    "![SFFS](images/SFFS.PNG)\n",
    "\n",
    "### Implementation of the Sequential Forward Floating Selection Algorithm\n",
    "\n",
    "SFFS (Sequential Forward Floating Selection) yielding a subset of d features, with optional search-restricting parameter ∆ ∈ [0, D − d]:\n",
    "1. Start with X0 = ∅, k = 0.\n",
    "2. Xk+1 = ADD(Xk), k = k + 1.\n",
    "3. Repeat Xk−1 = REMOVE(Xk), k = k − 1 as long as it improves solutions already known for the lower k.\n",
    "4. If k < d + ∆ go to 2.\n",
    "\n",
    "![SFFS2](images/SFFS_algorithm.PNG)\n",
    "\n",
    "The goal of SFFS is to gradually build a feature subset that optimizes a specific evaluation criterion while allowing for the removal of previously selected features if it benefits the overall performance.\n",
    "\n",
    "**Reference**: Somol, Petr & Novovicova, Jana & Pudil, Pavel. \"Efficient Feature Subset Selection and Subset Size Optimization\", in book: \"Pattern Recognition Recent Advances\", February 2010. DOI: [10.5772/9356](https://doi.org/10.5772/9356)."
   ]
  },
  {
   "cell_type": "code",
   "id": "f5d13b74f6774665",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def selection_sffs(df, selectedFeatures, remainingFeatures, target,\n",
    "                   metrics, maxIteration=1, discreteDf=None, beta=0.5):\n",
    "    metrics = checkMetrics(metrics, allowedMetrics)\n",
    "    selectedFeaturesScore = []\n",
    "    removedFeatures = []\n",
    "\n",
    "    while len(selectedFeatures) < maxIteration and remainingFeatures:\n",
    "        # Forward step (add best feature)\n",
    "        selectedFeatures, featScore, remainingFeatures = selection_sfs(\n",
    "            df, selectedFeatures, remainingFeatures, target,\n",
    "            metrics, maxIteration=1, discreteDf=discreteDf, beta=beta)\n",
    "        selectedFeaturesScore.extend(featScore)\n",
    "\n",
    "        # Floating backward steps\n",
    "        improved = True\n",
    "        while improved and len(selectedFeatures) > 1:\n",
    "            newSelected, eliminated, elimScore = selection_sbs(\n",
    "                df, selectedFeatures, target,\n",
    "                metrics, maxIteration=1, discreteDf=discreteDf, beta=beta)\n",
    "            if eliminated:\n",
    "                selectedFeatures = newSelected\n",
    "                remainingFeatures.extend(eliminated)\n",
    "                removedFeatures.extend(eliminated)\n",
    "                selectedFeaturesScore.extend(elimScore)\n",
    "            else:\n",
    "                improved = False\n",
    "\n",
    "    return selectedFeatures, selectedFeaturesScore, remainingFeatures, removedFeatures"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8a57335304e8e474",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sequential Forward Floating Selection (SFFS) with variance as evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "69b4299e9fe6b997",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures = selection_sffs(df, [], df.columns[:-1].tolist(), df.columns[-1], metrics=allowedMetrics[0], maxIteration= 2)\n",
    "\n",
    "printScoreDetails(selectedFeatures, selectedFeaturesScore, eliminatedFeatures)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2717ee42f3df2944",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sequential Forward Floating Selection (SFFS) with correlation as evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "b3c8e43e0498a649",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures = selection_sffs(df, [], df.columns[:-1].tolist(), df.columns[-1], metrics=allowedMetrics[1], maxIteration= 2)\n",
    "\n",
    "printScoreDetails(selectedFeatures, selectedFeaturesScore, eliminatedFeatures)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a3feb857de3518fa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sequential Forward Floating Selection (SFFS) with Correlation Feature Selection as evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "9a890367f8d10e6b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures = selection_sffs(df, [], df.columns[:-1].tolist(), df.columns[-1], metrics=allowedMetrics[2], maxIteration= 2)\n",
    "\n",
    "printScoreDetails(selectedFeatures, selectedFeaturesScore, eliminatedFeatures)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aeab83873c3aa123",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sequential Forward Floating Selection (SFFS) with Mutual Information Feature Selection as evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "702519052911abc6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "selectedFeatures, selectedFeaturesScore, remainingFeatures, eliminatedFeatures = selection_sffs(df, [], df.columns[:-1].tolist(), df.columns[-1], metrics=allowedMetrics[3], maxIteration= 2)\n",
    "\n",
    "printScoreDetails(selectedFeatures, selectedFeaturesScore, eliminatedFeatures)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f004ee9aabc6f74f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sequential Backward Floating Selection (SBFS)\n",
    "\n",
    "Sequential Backward Floating Selection) yielding a subset of d features, with optional search-restricting parameter Δ ∈ [0, d]:\n",
    "\n",
    "![SFFS](images/SBFS_algorithm.PNG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
